{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "FkvBs2kkk0VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --q"
      ],
      "metadata": {
        "id": "NCMWIuGakslU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "Itel1B0OktLk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = load_dataset(\"Bahareh0281/liveness_images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nlXJUhbkxKx",
        "outputId": "1244523c-bf68-4349-94d3-5f296d3c85ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset['train'][7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W66msnCyk_tx",
        "outputId": "544e906f-ea8f-4264-81e8-2c716dec21ca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256>,\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Libraries"
      ],
      "metadata": {
        "id": "lMhDO8JIixJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "URBYLMbzh7fL"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage import measure\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction Functions"
      ],
      "metadata": {
        "id": "SNoHJHh8jESM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "radius = 3\n",
        "n_points = 8 * radius\n",
        "\n",
        "def compute_fourier_transform(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    f = np.fft.fft2(gray)\n",
        "    fshift = np.fft.fftshift(f)\n",
        "    magnitude_spectrum = 20 * np.log(np.abs(fshift))\n",
        "    return magnitude_spectrum\n",
        "\n",
        "def compute_lbp(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    lbp = local_binary_pattern(gray, n_points, radius, method=\"uniform\")\n",
        "    return lbp\n",
        "\n",
        "def compute_depth(image):\n",
        "    # به عنوان مثال از کانال آبی برای تخمین عمق استفاده می‌کنیم\n",
        "    depth = image[:, :, 2]\n",
        "    return depth\n",
        "\n",
        "def extract_statistical_features(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    mean = np.mean(gray)\n",
        "    std_dev = np.std(gray)\n",
        "    skewness = np.mean((gray - mean) ** 3) / (std_dev ** 3)\n",
        "    kurtosis = np.mean((gray - mean) ** 4) / (std_dev ** 4)\n",
        "    entropy = measure.shannon_entropy(gray)\n",
        "    return mean, std_dev, skewness, kurtosis, entropy"
      ],
      "metadata": {
        "id": "dot437z0jBhz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Input Images"
      ],
      "metadata": {
        "id": "Jsan8-dljK-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images(dataset, num=None):\n",
        "    train_images_features = []\n",
        "    train_images_labels = []\n",
        "    train_data = dataset['train'][:num] if num else dataset['train']\n",
        "\n",
        "    for idx, data in enumerate(train_data):\n",
        "        print(f\"Processing image {idx+1}/{len(train_data)}\")  # Debug: Track progress\n",
        "        if isinstance(data, dict) and 'image' in data and 'label' in data:\n",
        "            img = data['image']\n",
        "            if img is not None:\n",
        "                img = np.array(img)\n",
        "                # Extract frequency features\n",
        "                magnitude_spectrum = compute_fourier_transform(img)\n",
        "                magnitude_spectrum_mean = np.mean(magnitude_spectrum)\n",
        "\n",
        "                # Extract LBP features\n",
        "                lbp = compute_lbp(img)\n",
        "                lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
        "                lbp_hist = lbp_hist / lbp_hist.sum()\n",
        "\n",
        "                # Extract depth features\n",
        "                depth = compute_depth(img)\n",
        "                depth_mean = np.mean(depth)\n",
        "\n",
        "                # Extract statistical features\n",
        "                mean, std_dev, skewness, kurtosis, entropy = extract_statistical_features(img)\n",
        "                combined_features = np.concatenate([\n",
        "                    [magnitude_spectrum_mean],\n",
        "                    lbp_hist,\n",
        "                    [depth_mean, mean, std_dev, skewness, kurtosis, entropy]\n",
        "                ])\n",
        "                train_images_features.append(combined_features)\n",
        "                train_images_labels.append(data['label'])\n",
        "        else:\n",
        "            print(f\"Invalid data format at index {idx}: {data}\")  # Debug: Check for incorrect formats\n",
        "\n",
        "    return np.array(train_images_features), np.array(train_images_labels)"
      ],
      "metadata": {
        "id": "U8dxUvsXi7XG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Dataset['train'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkl0BgNdtzNS",
        "outputId": "35969f98-1e95-400c-dad8-0f37ae0c420c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6427"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images(dataset, num=0):\n",
        "    train_images_features = []\n",
        "    train_images_labels = []\n",
        "    if num == 0:\n",
        "        num = len(dataset['train'])\n",
        "\n",
        "    for i in range(num):\n",
        "        img = dataset['train'][i]['image']\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)  # Convert PIL image to NumPy array\n",
        "\n",
        "            # Extract frequency features\n",
        "            magnitude_spectrum = compute_fourier_transform(img)\n",
        "            magnitude_spectrum_resized = cv2.resize(magnitude_spectrum, (64, 64))\n",
        "\n",
        "            # Extract LBP features\n",
        "            lbp = compute_lbp(img)\n",
        "            lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
        "            lbp_hist_normalized = lbp_hist / lbp_hist.sum()\n",
        "            lbp_hist_resized = cv2.resize(lbp_hist_normalized.reshape(-1, 1), (64, 64))\n",
        "\n",
        "            # Extract statistical features\n",
        "            mean, std_dev, skewness, kurtosis, entropy = extract_statistical_features(img)\n",
        "            statistical_features = np.array([mean, std_dev, skewness, kurtosis, entropy])\n",
        "            statistical_features_resized = cv2.resize(statistical_features.reshape(-1, 1), (64, 64))\n",
        "\n",
        "            # Combine features into a 3D array\n",
        "            combined_features = np.stack([\n",
        "                magnitude_spectrum_resized,\n",
        "                lbp_hist_resized,\n",
        "                statistical_features_resized\n",
        "            ], axis=-1)\n",
        "\n",
        "            train_images_features.append(combined_features)\n",
        "            train_images_labels.append(dataset['train'][i]['label'])\n",
        "\n",
        "    return np.array(train_images_features), np.array(train_images_labels)"
      ],
      "metadata": {
        "id": "z87X257BthVT"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# پردازش تصاویر و استخراج ویژگی‌ها\n",
        "train_images_features,  train_images_labels = process_images(Dataset, 3000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up51-JBOi6K8",
        "outputId": "3fdcb58e-f901-461e-d70a-3e784712bd57"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-11f85dd407e5>:8: RuntimeWarning: divide by zero encountered in log\n",
            "  magnitude_spectrum = 20 * np.log(np.abs(fshift))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_images_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esqMUsDfs1Vz",
        "outputId": "f2e465de-513a-4bbb-fdac-ea7cae4c5d0d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C3fYXFByuPK",
        "outputId": "3ec9aa86-65e7-400b-ee28-220781d446b3"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 64, 64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_features[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVbEpCHpy196",
        "outputId": "5ac34939-c4c8-4ab6-8fb1-4951bb74db5f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[9.98781304e+01, 1.70135498e-02, 1.23999863e+02],\n",
              "        [1.03112881e+02, 1.70135498e-02, 1.23999863e+02],\n",
              "        [1.00401371e+02, 1.70135498e-02, 1.23999863e+02],\n",
              "        ...,\n",
              "        [8.08424041e+01, 1.70135498e-02, 1.23999863e+02],\n",
              "        [8.49438029e+01, 1.70135498e-02, 1.23999863e+02],\n",
              "        [9.55299601e+01, 1.70135498e-02, 1.23999863e+02]],\n",
              "\n",
              "       [[7.85450649e+01, 1.65896416e-02, 1.23999863e+02],\n",
              "        [8.88281131e+01, 1.65896416e-02, 1.23999863e+02],\n",
              "        [8.47440767e+01, 1.65896416e-02, 1.23999863e+02],\n",
              "        ...,\n",
              "        [9.16315104e+01, 1.65896416e-02, 1.23999863e+02],\n",
              "        [9.06426121e+01, 1.65896416e-02, 1.23999863e+02],\n",
              "        [8.25498088e+01, 1.65896416e-02, 1.23999863e+02]],\n",
              "\n",
              "       [[9.15188864e+01, 1.50151253e-02, 1.23999863e+02],\n",
              "        [9.45883978e+01, 1.50151253e-02, 1.23999863e+02],\n",
              "        [9.23813503e+01, 1.50151253e-02, 1.23999863e+02],\n",
              "        ...,\n",
              "        [9.37271837e+01, 1.50151253e-02, 1.23999863e+02],\n",
              "        [9.23618173e+01, 1.50151253e-02, 1.23999863e+02],\n",
              "        [9.04500895e+01, 1.50151253e-02, 1.23999863e+02]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[8.18726194e+01, 1.72089338e-01, 6.84012221e+00],\n",
              "        [9.39470200e+01, 1.72089338e-01, 6.84012221e+00],\n",
              "        [8.42057294e+01, 1.72089338e-01, 6.84012221e+00],\n",
              "        ...,\n",
              "        [9.14947383e+01, 1.72089338e-01, 6.84012221e+00],\n",
              "        [7.99644254e+01, 1.72089338e-01, 6.84012221e+00],\n",
              "        [9.64360706e+01, 1.72089338e-01, 6.84012221e+00]],\n",
              "\n",
              "       [[7.67963766e+01, 2.15115786e-01, 6.84012221e+00],\n",
              "        [8.99967501e+01, 2.15115786e-01, 6.84012221e+00],\n",
              "        [8.08708981e+01, 2.15115786e-01, 6.84012221e+00],\n",
              "        ...,\n",
              "        [8.95057065e+01, 2.15115786e-01, 6.84012221e+00],\n",
              "        [8.59257870e+01, 2.15115786e-01, 6.84012221e+00],\n",
              "        [7.89247608e+01, 2.15115786e-01, 6.84012221e+00]],\n",
              "\n",
              "       [[9.62452804e+01, 2.26699829e-01, 6.84012221e+00],\n",
              "        [8.69844970e+01, 2.26699829e-01, 6.84012221e+00],\n",
              "        [9.12226090e+01, 2.26699829e-01, 6.84012221e+00],\n",
              "        ...,\n",
              "        [9.74845177e+01, 2.26699829e-01, 6.84012221e+00],\n",
              "        [9.76303721e+01, 2.26699829e-01, 6.84012221e+00],\n",
              "        [9.74363104e+01, 2.26699829e-01, 6.84012221e+00]]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_images_features[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBECxZD6zG0n",
        "outputId": "8e84c5ec-78c0-4790-c48e-99a7d9577566"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_images_features[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-vc9TiazIbY",
        "outputId": "dc7193b6-d759-4bc1-e435-58fc11f07511"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split training dataset and prepare it for train process"
      ],
      "metadata": {
        "id": "5VY-vRmm7N6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert depth features to a numpy array\n",
        "features = np.array(train_images_features)\n",
        "labels = np.array(train_images_labels)\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, num_classes=2)\n",
        "y_test = to_categorical(y_test, num_classes=2)\n"
      ],
      "metadata": {
        "id": "aJZ792Fe0JLs"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create CNN Model and Train it"
      ],
      "metadata": {
        "id": "oBE5VM3g7XFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7x7f9i33lS4",
        "outputId": "fe2a75ff-042f-442a-d694-14ae1ba25818"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "60/60 [==============================] - 6s 11ms/step - loss: 5.9239 - accuracy: 0.7688 - val_loss: 0.6173 - val_accuracy: 0.8271\n",
            "Epoch 2/20\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5678 - accuracy: 0.7948 - val_loss: 0.4972 - val_accuracy: 0.8271\n",
            "Epoch 3/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5170 - accuracy: 0.7948 - val_loss: 0.4695 - val_accuracy: 0.8271\n",
            "Epoch 4/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5114 - accuracy: 0.7948 - val_loss: 0.4657 - val_accuracy: 0.8271\n",
            "Epoch 5/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5091 - accuracy: 0.7948 - val_loss: 0.4637 - val_accuracy: 0.8271\n",
            "Epoch 6/20\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5118 - accuracy: 0.7948 - val_loss: 0.4639 - val_accuracy: 0.8271\n",
            "Epoch 7/20\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.5078 - accuracy: 0.7948 - val_loss: 0.4642 - val_accuracy: 0.8271\n",
            "Epoch 8/20\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5114 - accuracy: 0.7948 - val_loss: 0.4644 - val_accuracy: 0.8271\n",
            "Epoch 9/20\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.5119 - accuracy: 0.7948 - val_loss: 0.4633 - val_accuracy: 0.8271\n",
            "Epoch 10/20\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.5106 - accuracy: 0.7948 - val_loss: 0.4640 - val_accuracy: 0.8271\n",
            "Epoch 11/20\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5072 - accuracy: 0.7948 - val_loss: 0.4637 - val_accuracy: 0.8271\n",
            "Epoch 12/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5108 - accuracy: 0.7948 - val_loss: 0.4643 - val_accuracy: 0.8271\n",
            "Epoch 13/20\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5115 - accuracy: 0.7948 - val_loss: 0.4643 - val_accuracy: 0.8271\n",
            "Epoch 14/20\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5096 - accuracy: 0.7948 - val_loss: 0.4640 - val_accuracy: 0.8271\n",
            "Epoch 15/20\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5103 - accuracy: 0.7948 - val_loss: 0.4659 - val_accuracy: 0.8271\n",
            "Epoch 16/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5114 - accuracy: 0.7948 - val_loss: 0.4647 - val_accuracy: 0.8271\n",
            "Epoch 17/20\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5131 - accuracy: 0.7948 - val_loss: 0.4647 - val_accuracy: 0.8271\n",
            "Epoch 18/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5147 - accuracy: 0.7948 - val_loss: 0.4654 - val_accuracy: 0.8271\n",
            "Epoch 19/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5111 - accuracy: 0.7948 - val_loss: 0.4646 - val_accuracy: 0.8271\n",
            "Epoch 20/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.5107 - accuracy: 0.7948 - val_loss: 0.4642 - val_accuracy: 0.8271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate on dataset tests"
      ],
      "metadata": {
        "id": "z-VAc5m57ei2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DCO0A3o4IV2",
        "outputId": "04417639-ac0b-4b39-8c85-6dee0fde272a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 0s 10ms/step - loss: 0.4871 - accuracy: 0.8100\n",
            "Test Accuracy: 81.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load test videos"
      ],
      "metadata": {
        "id": "tfF_mfOt7ksY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "file_id = '1a5R5h05hCyw9PzIBhSjy2jLL3dSFy2xA'\n",
        "destination = '/content/dataset.zip'  # Path where the file will be saved\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', destination, quiet=False)\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(destination, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPgJ2aLO5MkB",
        "outputId": "1e6c60ad-32d1-4b78-a2b3-99391eaf9e1c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1a5R5h05hCyw9PzIBhSjy2jLL3dSFy2xA\n",
            "From (redirected): https://drive.google.com/uc?id=1a5R5h05hCyw9PzIBhSjy2jLL3dSFy2xA&confirm=t&uuid=7899b5c3-8e3e-4dec-b2e6-13adf3a08e14\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 377M/377M [00:05<00:00, 64.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate random frames from each video"
      ],
      "metadata": {
        "id": "Zy4f0V9B7osL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def extract_frames(video_path, save_path, label, test):\n",
        "    # Open the video file\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Select one random frame\n",
        "    random_frame = random.randint(0, frame_count - 1)\n",
        "\n",
        "    # Set the position of the video to the selected frame\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, random_frame)\n",
        "    success, frame = video.read()\n",
        "\n",
        "    # If the frame was successfully read, save it\n",
        "    if success:\n",
        "        frame_path = os.path.join(save_path, f\"{label}_{random_frame}.jpg\")\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "\n",
        "        # Convert the frame to a PIL image\n",
        "        pil_image = Image.open(frame_path)\n",
        "\n",
        "        # Save the image and label to the dictionary\n",
        "        test.append({'image': pil_image, 'label': label})\n",
        "\n",
        "    # Release the video file\n",
        "    video.release()"
      ],
      "metadata": {
        "id": "rgdA0sFM4qgL"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "fake_test_videos_path = '/content/dataset/fake/test'\n",
        "real_test_videos_path = '/content/dataset/real/test'\n",
        "\n",
        "save_frames_path = '/content/extracted_frames/test'\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(save_frames_path):\n",
        "    os.makedirs(save_frames_path)\n",
        "\n",
        "# Create a list to hold the dictionary entries\n",
        "test = []\n",
        "\n",
        "# Iterate over fake videos and extract frames\n",
        "for fake_video_file in os.listdir(fake_test_videos_path):\n",
        "    fake_video_path = os.path.join(fake_test_videos_path, fake_video_file)\n",
        "    extract_frames(fake_video_path, save_frames_path, 0, test)\n",
        "\n",
        "# Iterate over real videos and extract frames\n",
        "for real_video_file in os.listdir(real_test_videos_path):\n",
        "    real_video_path = os.path.join(real_test_videos_path, real_video_file)\n",
        "    extract_frames(real_video_path, save_frames_path, 1, test)"
      ],
      "metadata": {
        "id": "_bRG9QQu4rWX"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract features from each frame"
      ],
      "metadata": {
        "id": "l-Y77Zij704f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images_from_directory(directory, num=0):\n",
        "    train_images_features = []\n",
        "    train_images_labels = []\n",
        "\n",
        "    image_files = [f for f in os.listdir(directory) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    if num == 0 or num > len(image_files):\n",
        "        num = len(image_files)\n",
        "\n",
        "    for i, file_name in enumerate(image_files[:num]):\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "        try:\n",
        "            img = Image.open(file_path)\n",
        "            img = np.array(img)  # Convert PIL image to NumPy array\n",
        "\n",
        "            # Extract frequency features\n",
        "            magnitude_spectrum = compute_fourier_transform(img)\n",
        "            magnitude_spectrum_resized = cv2.resize(magnitude_spectrum, (64, 64))\n",
        "\n",
        "            # Extract LBP features\n",
        "            lbp = compute_lbp(img)\n",
        "            lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
        "            lbp_hist_normalized = lbp_hist / lbp_hist.sum()\n",
        "            lbp_hist_resized = cv2.resize(lbp_hist_normalized.reshape(-1, 1), (64, 64))\n",
        "\n",
        "            # Extract statistical features\n",
        "            mean, std_dev, skewness, kurtosis, entropy = extract_statistical_features(img)\n",
        "            statistical_features = np.array([mean, std_dev, skewness, kurtosis, entropy])\n",
        "            statistical_features_resized = cv2.resize(statistical_features.reshape(-1, 1), (64, 64))\n",
        "\n",
        "            # Combine features into a 3D array\n",
        "            combined_features = np.stack([\n",
        "                magnitude_spectrum_resized,\n",
        "                lbp_hist_resized,\n",
        "                statistical_features_resized\n",
        "            ], axis=-1)\n",
        "\n",
        "            train_images_features.append(combined_features)\n",
        "            train_images_labels.append(0)  # Change this if you have actual labels\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "    return np.array(train_images_features), np.array(train_images_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "zgwrJgz74uKi"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract features from each frame and convert labels to one-hot form"
      ],
      "metadata": {
        "id": "rqtnp6tY77S4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/content/extracted_frames/test'\n",
        "test_frames_features, test_frames_labels = process_images_from_directory(directory)\n",
        "\n",
        "test_frames_labels = to_categorical(test_frames_labels, num_classes=2)"
      ],
      "metadata": {
        "id": "RkPimjUo6GW9"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model on test set"
      ],
      "metadata": {
        "id": "z7UunJdx8FEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlfFZ28Q6qbg",
        "outputId": "a6b447ce-2c93-4c2a-9d7c-7e51bf709633"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 0s 4ms/step - loss: 0.4871 - accuracy: 0.8100\n",
            "Test Accuracy: 81.00%\n"
          ]
        }
      ]
    }
  ]
}