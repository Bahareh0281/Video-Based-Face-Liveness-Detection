{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "FkvBs2kkk0VE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --q"
      ],
      "metadata": {
        "id": "NCMWIuGakslU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "Itel1B0OktLk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset = load_dataset(\"Bahareh0281/liveness_images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nlXJUhbkxKx",
        "outputId": "1244523c-bf68-4349-94d3-5f296d3c85ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset['train'][7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W66msnCyk_tx",
        "outputId": "544e906f-ea8f-4264-81e8-2c716dec21ca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256>,\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Libraries"
      ],
      "metadata": {
        "id": "lMhDO8JIixJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "URBYLMbzh7fL"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage import measure\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction Functions"
      ],
      "metadata": {
        "id": "SNoHJHh8jESM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "radius = 3\n",
        "n_points = 8 * radius\n",
        "\n",
        "def compute_fourier_transform(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    f = np.fft.fft2(gray)\n",
        "    fshift = np.fft.fftshift(f)\n",
        "    magnitude_spectrum = 20 * np.log(np.abs(fshift))\n",
        "    return magnitude_spectrum\n",
        "\n",
        "def compute_lbp(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    lbp = local_binary_pattern(gray, n_points, radius, method=\"uniform\")\n",
        "    return lbp\n",
        "\n",
        "def compute_depth(image):\n",
        "    # به عنوان مثال از کانال آبی برای تخمین عمق استفاده می‌کنیم\n",
        "    depth = image[:, :, 2]\n",
        "    return depth\n",
        "\n",
        "def extract_statistical_features(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    mean = np.mean(gray)\n",
        "    std_dev = np.std(gray)\n",
        "    skewness = np.mean((gray - mean) ** 3) / (std_dev ** 3)\n",
        "    kurtosis = np.mean((gray - mean) ** 4) / (std_dev ** 4)\n",
        "    entropy = measure.shannon_entropy(gray)\n",
        "    return mean, std_dev, skewness, kurtosis, entropy"
      ],
      "metadata": {
        "id": "dot437z0jBhz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Input Images"
      ],
      "metadata": {
        "id": "Jsan8-dljK-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images(dataset, num=None):\n",
        "    train_images_features = []\n",
        "    train_images_labels = []\n",
        "    train_data = dataset['train'][:num] if num else dataset['train']\n",
        "\n",
        "    for idx, data in enumerate(train_data):\n",
        "        print(f\"Processing image {idx+1}/{len(train_data)}\")  # Debug: Track progress\n",
        "        if isinstance(data, dict) and 'image' in data and 'label' in data:\n",
        "            img = data['image']\n",
        "            if img is not None:\n",
        "                img = np.array(img)\n",
        "                # Extract frequency features\n",
        "                magnitude_spectrum = compute_fourier_transform(img)\n",
        "                magnitude_spectrum_mean = np.mean(magnitude_spectrum)\n",
        "\n",
        "                # Extract LBP features\n",
        "                lbp = compute_lbp(img)\n",
        "                lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
        "                lbp_hist = lbp_hist / lbp_hist.sum()\n",
        "\n",
        "                # Extract depth features\n",
        "                depth = compute_depth(img)\n",
        "                depth_mean = np.mean(depth)\n",
        "\n",
        "                # Extract statistical features\n",
        "                mean, std_dev, skewness, kurtosis, entropy = extract_statistical_features(img)\n",
        "                combined_features = np.concatenate([\n",
        "                    [magnitude_spectrum_mean],\n",
        "                    lbp_hist,\n",
        "                    [depth_mean, mean, std_dev, skewness, kurtosis, entropy]\n",
        "                ])\n",
        "                train_images_features.append(combined_features)\n",
        "                train_images_labels.append(data['label'])\n",
        "        else:\n",
        "            print(f\"Invalid data format at index {idx}: {data}\")  # Debug: Check for incorrect formats\n",
        "\n",
        "    return np.array(train_images_features), np.array(train_images_labels)"
      ],
      "metadata": {
        "id": "U8dxUvsXi7XG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(Dataset['train'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkl0BgNdtzNS",
        "outputId": "35969f98-1e95-400c-dad8-0f37ae0c420c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6427"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images(dataset, num=0):\n",
        "    train_images_features = []\n",
        "    train_images_labels = []\n",
        "    if num == 0:\n",
        "        num = len(dataset['train'])\n",
        "\n",
        "    for i in range(num):\n",
        "        img = dataset['train'][i]['image']\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)  # Convert PIL image to NumPy array\n",
        "\n",
        "            # Extract frequency features\n",
        "            magnitude_spectrum = compute_fourier_transform(img)\n",
        "            magnitude_spectrum_resized = cv2.resize(magnitude_spectrum, (64, 64))\n",
        "\n",
        "            # Extract LBP features\n",
        "            lbp = compute_lbp(img)\n",
        "            lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
        "            lbp_hist_normalized = lbp_hist / lbp_hist.sum()\n",
        "            lbp_hist_resized = cv2.resize(lbp_hist_normalized.reshape(-1, 1), (64, 64))\n",
        "\n",
        "            # Extract statistical features\n",
        "            mean, std_dev, skewness, kurtosis, entropy = extract_statistical_features(img)\n",
        "            statistical_features = np.array([mean, std_dev, skewness, kurtosis, entropy])\n",
        "            statistical_features_resized = cv2.resize(statistical_features.reshape(-1, 1), (64, 64))\n",
        "\n",
        "            # Combine features into a 3D array\n",
        "            combined_features = np.stack([\n",
        "                magnitude_spectrum_resized,\n",
        "                lbp_hist_resized,\n",
        "                statistical_features_resized\n",
        "            ], axis=-1)\n",
        "\n",
        "            train_images_features.append(combined_features)\n",
        "            train_images_labels.append(dataset['train'][i]['label'])\n",
        "\n",
        "    return np.array(train_images_features), np.array(train_images_labels)"
      ],
      "metadata": {
        "id": "z87X257BthVT"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# پردازش تصاویر و استخراج ویژگی‌ها\n",
        "train_images_features,  train_images_labels = process_images(Dataset, 3000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up51-JBOi6K8",
        "outputId": "3fdcb58e-f901-461e-d70a-3e784712bd57"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-11f85dd407e5>:8: RuntimeWarning: divide by zero encountered in log\n",
            "  magnitude_spectrum = 20 * np.log(np.abs(fshift))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_images_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esqMUsDfs1Vz",
        "outputId": "f2e465de-513a-4bbb-fdac-ea7cae4c5d0d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C3fYXFByuPK",
        "outputId": "3ec9aa86-65e7-400b-ee28-220781d446b3"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000, 64, 64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_features[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVbEpCHpy196",
        "outputId": "5ac34939-c4c8-4ab6-8fb1-4951bb74db5f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[9.98781304e+01, 1.70135498e-02, 1.23999863e+02],\n",
              "        [1.03112881e+02, 1.70135498e-02, 1.23999863e+02],\n",
              "        [1.00401371e+02, 1.70135498e-02, 1.23999863e+02],\n",
              "        ...,\n",
              "        [8.08424041e+01, 1.70135498e-02, 1.23999863e+02],\n",
              "        [8.49438029e+01, 1.70135498e-02, 1.23999863e+02],\n",
              "        [9.55299601e+01, 1.70135498e-02, 1.23999863e+02]],\n",
              "\n",
              "       [[7.85450649e+01, 1.65896416e-02, 1.23999863e+02],\n",
              "        [8.88281131e+01, 1.65896416e-02, 1.23999863e+02],\n",
              "        [8.47440767e+01, 1.65896416e-02, 1.23999863e+02],\n",
              "        ...,\n",
              "        [9.16315104e+01, 1.65896416e-02, 1.23999863e+02],\n",
              "        [9.06426121e+01, 1.65896416e-02, 1.23999863e+02],\n",
              "        [8.25498088e+01, 1.65896416e-02, 1.23999863e+02]],\n",
              "\n",
              "       [[9.15188864e+01, 1.50151253e-02, 1.23999863e+02],\n",
              "        [9.45883978e+01, 1.50151253e-02, 1.23999863e+02],\n",
              "        [9.23813503e+01, 1.50151253e-02, 1.23999863e+02],\n",
              "        ...,\n",
              "        [9.37271837e+01, 1.50151253e-02, 1.23999863e+02],\n",
              "        [9.23618173e+01, 1.50151253e-02, 1.23999863e+02],\n",
              "        [9.04500895e+01, 1.50151253e-02, 1.23999863e+02]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[8.18726194e+01, 1.72089338e-01, 6.84012221e+00],\n",
              "        [9.39470200e+01, 1.72089338e-01, 6.84012221e+00],\n",
              "        [8.42057294e+01, 1.72089338e-01, 6.84012221e+00],\n",
              "        ...,\n",
              "        [9.14947383e+01, 1.72089338e-01, 6.84012221e+00],\n",
              "        [7.99644254e+01, 1.72089338e-01, 6.84012221e+00],\n",
              "        [9.64360706e+01, 1.72089338e-01, 6.84012221e+00]],\n",
              "\n",
              "       [[7.67963766e+01, 2.15115786e-01, 6.84012221e+00],\n",
              "        [8.99967501e+01, 2.15115786e-01, 6.84012221e+00],\n",
              "        [8.08708981e+01, 2.15115786e-01, 6.84012221e+00],\n",
              "        ...,\n",
              "        [8.95057065e+01, 2.15115786e-01, 6.84012221e+00],\n",
              "        [8.59257870e+01, 2.15115786e-01, 6.84012221e+00],\n",
              "        [7.89247608e+01, 2.15115786e-01, 6.84012221e+00]],\n",
              "\n",
              "       [[9.62452804e+01, 2.26699829e-01, 6.84012221e+00],\n",
              "        [8.69844970e+01, 2.26699829e-01, 6.84012221e+00],\n",
              "        [9.12226090e+01, 2.26699829e-01, 6.84012221e+00],\n",
              "        ...,\n",
              "        [9.74845177e+01, 2.26699829e-01, 6.84012221e+00],\n",
              "        [9.76303721e+01, 2.26699829e-01, 6.84012221e+00],\n",
              "        [9.74363104e+01, 2.26699829e-01, 6.84012221e+00]]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_images_features[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBECxZD6zG0n",
        "outputId": "8e84c5ec-78c0-4790-c48e-99a7d9577566"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_images_features[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-vc9TiazIbY",
        "outputId": "dc7193b6-d759-4bc1-e435-58fc11f07511"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split training dataset and prepare it for train process"
      ],
      "metadata": {
        "id": "5VY-vRmm7N6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert depth features to a numpy array\n",
        "features = np.array(train_images_features)\n",
        "labels = np.array(train_images_labels)\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, num_classes=2)\n",
        "y_test = to_categorical(y_test, num_classes=2)\n"
      ],
      "metadata": {
        "id": "aJZ792Fe0JLs"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create CNN Model and Train it"
      ],
      "metadata": {
        "id": "oBE5VM3g7XFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7x7f9i33lS4",
        "outputId": "cfe62825-4f86-4dac-d21b-203ce021dfdb"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "60/60 [==============================] - 2s 11ms/step - loss: 10.7011 - accuracy: 0.7349 - val_loss: 0.4753 - val_accuracy: 0.8271\n",
            "Epoch 2/20\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.5150 - accuracy: 0.7943 - val_loss: 0.4210 - val_accuracy: 0.8271\n",
            "Epoch 3/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4814 - accuracy: 0.7964 - val_loss: 0.4234 - val_accuracy: 0.8250\n",
            "Epoch 4/20\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.4670 - accuracy: 0.7974 - val_loss: 0.4367 - val_accuracy: 0.8271\n",
            "Epoch 5/20\n",
            "60/60 [==============================] - 0s 8ms/step - loss: 0.4695 - accuracy: 0.7964 - val_loss: 0.4154 - val_accuracy: 0.8208\n",
            "Epoch 6/20\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.4730 - accuracy: 0.7995 - val_loss: 0.4567 - val_accuracy: 0.8062\n",
            "Epoch 7/20\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.4549 - accuracy: 0.8062 - val_loss: 0.4291 - val_accuracy: 0.8333\n",
            "Epoch 8/20\n",
            "60/60 [==============================] - 1s 9ms/step - loss: 0.4515 - accuracy: 0.8161 - val_loss: 0.3846 - val_accuracy: 0.8583\n",
            "Epoch 9/20\n",
            "60/60 [==============================] - 1s 8ms/step - loss: 0.4402 - accuracy: 0.8208 - val_loss: 0.4155 - val_accuracy: 0.8604\n",
            "Epoch 10/20\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.4337 - accuracy: 0.8240 - val_loss: 0.3996 - val_accuracy: 0.8562\n",
            "Epoch 11/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4274 - accuracy: 0.8313 - val_loss: 0.3846 - val_accuracy: 0.8562\n",
            "Epoch 12/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4305 - accuracy: 0.8214 - val_loss: 0.4475 - val_accuracy: 0.7979\n",
            "Epoch 13/20\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.4441 - accuracy: 0.8245 - val_loss: 0.3988 - val_accuracy: 0.8562\n",
            "Epoch 14/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4540 - accuracy: 0.8130 - val_loss: 0.3696 - val_accuracy: 0.8562\n",
            "Epoch 15/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4284 - accuracy: 0.8240 - val_loss: 0.3610 - val_accuracy: 0.8604\n",
            "Epoch 16/20\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.4109 - accuracy: 0.8333 - val_loss: 0.3731 - val_accuracy: 0.8521\n",
            "Epoch 17/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4158 - accuracy: 0.8281 - val_loss: 0.3703 - val_accuracy: 0.8562\n",
            "Epoch 18/20\n",
            "60/60 [==============================] - 0s 7ms/step - loss: 0.4133 - accuracy: 0.8313 - val_loss: 0.3589 - val_accuracy: 0.8646\n",
            "Epoch 19/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4103 - accuracy: 0.8349 - val_loss: 0.3561 - val_accuracy: 0.8604\n",
            "Epoch 20/20\n",
            "60/60 [==============================] - 0s 6ms/step - loss: 0.4039 - accuracy: 0.8328 - val_loss: 0.3857 - val_accuracy: 0.8396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate on dataset tests"
      ],
      "metadata": {
        "id": "z-VAc5m57ei2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DCO0A3o4IV2",
        "outputId": "df978d6d-a3a6-4362-da37-d50033ae21e3"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 0s 4ms/step - loss: 0.4062 - accuracy: 0.8317\n",
            "Test Accuracy: 83.17%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load test videos"
      ],
      "metadata": {
        "id": "tfF_mfOt7ksY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "file_id = '1a5R5h05hCyw9PzIBhSjy2jLL3dSFy2xA'\n",
        "destination = '/content/dataset.zip'  # Path where the file will be saved\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', destination, quiet=False)\n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(destination, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPgJ2aLO5MkB",
        "outputId": "a0ffad69-f9bb-4168-8995-f6c19e627d14"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1a5R5h05hCyw9PzIBhSjy2jLL3dSFy2xA\n",
            "From (redirected): https://drive.google.com/uc?id=1a5R5h05hCyw9PzIBhSjy2jLL3dSFy2xA&confirm=t&uuid=b8ca89d8-b9bb-4ad1-8d07-8a514b991c62\n",
            "To: /content/dataset.zip\n",
            "100%|██████████| 377M/377M [00:07<00:00, 48.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate random frames from each video"
      ],
      "metadata": {
        "id": "Zy4f0V9B7osL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "def extract_frames(video_path, save_path, label, test):\n",
        "    # Open the video file\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Select one random frame\n",
        "    random_frame = random.randint(0, frame_count - 1)\n",
        "\n",
        "    # Set the position of the video to the selected frame\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, random_frame)\n",
        "    success, frame = video.read()\n",
        "\n",
        "    # If the frame was successfully read, save it\n",
        "    if success:\n",
        "        frame_path = os.path.join(save_path, f\"{label}_{random_frame}.jpg\")\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "\n",
        "        # Convert the frame to a PIL image\n",
        "        pil_image = Image.open(frame_path)\n",
        "\n",
        "        # Save the image and label to the dictionary\n",
        "        test.append({'image': pil_image, 'label': label})\n",
        "\n",
        "    # Release the video file\n",
        "    video.release()"
      ],
      "metadata": {
        "id": "rgdA0sFM4qgL"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "fake_test_videos_path = '/content/dataset/fake/test'\n",
        "real_test_videos_path = '/content/dataset/real/test'\n",
        "\n",
        "save_frames_path = '/content/extracted_frames/test'\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(save_frames_path):\n",
        "    os.makedirs(save_frames_path)\n",
        "\n",
        "# Create a list to hold the dictionary entries\n",
        "test = []\n",
        "\n",
        "# Iterate over fake videos and extract frames\n",
        "for fake_video_file in os.listdir(fake_test_videos_path):\n",
        "    fake_video_path = os.path.join(fake_test_videos_path, fake_video_file)\n",
        "    extract_frames(fake_video_path, save_frames_path, 0, test)\n",
        "\n",
        "# Iterate over real videos and extract frames\n",
        "for real_video_file in os.listdir(real_test_videos_path):\n",
        "    real_video_path = os.path.join(real_test_videos_path, real_video_file)\n",
        "    extract_frames(real_video_path, save_frames_path, 1, test)"
      ],
      "metadata": {
        "id": "_bRG9QQu4rWX"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract features from each frame"
      ],
      "metadata": {
        "id": "l-Y77Zij704f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_images_from_directory(directory, num=0):\n",
        "    train_images_features = []\n",
        "    train_images_labels = []\n",
        "\n",
        "    image_files = [f for f in os.listdir(directory) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    if num == 0 or num > len(image_files):\n",
        "        num = len(image_files)\n",
        "\n",
        "    for i, file_name in enumerate(image_files[:num]):\n",
        "        file_path = os.path.join(directory, file_name)\n",
        "        try:\n",
        "            img = Image.open(file_path)\n",
        "            img = np.array(img)  # Convert PIL image to NumPy array\n",
        "\n",
        "            # Extract frequency features\n",
        "            magnitude_spectrum = compute_fourier_transform(img)\n",
        "            magnitude_spectrum_resized = cv2.resize(magnitude_spectrum, (64, 64))\n",
        "\n",
        "            # Extract LBP features\n",
        "            lbp = compute_lbp(img)\n",
        "            lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
        "            lbp_hist_normalized = lbp_hist / lbp_hist.sum()\n",
        "            lbp_hist_resized = cv2.resize(lbp_hist_normalized.reshape(-1, 1), (64, 64))\n",
        "\n",
        "            # Extract statistical features\n",
        "            mean, std_dev, skewness, kurtosis, entropy = extract_statistical_features(img)\n",
        "            statistical_features = np.array([mean, std_dev, skewness, kurtosis, entropy])\n",
        "            statistical_features_resized = cv2.resize(statistical_features.reshape(-1, 1), (64, 64))\n",
        "\n",
        "            # Combine features into a 3D array\n",
        "            combined_features = np.stack([\n",
        "                magnitude_spectrum_resized,\n",
        "                lbp_hist_resized,\n",
        "                statistical_features_resized\n",
        "            ], axis=-1)\n",
        "\n",
        "            train_images_features.append(combined_features)\n",
        "            train_images_labels.append(0)  # Change this if you have actual labels\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "    return np.array(train_images_features), np.array(train_images_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "zgwrJgz74uKi"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract features from each frame and convert labels to one-hot form"
      ],
      "metadata": {
        "id": "rqtnp6tY77S4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/content/extracted_frames/test'\n",
        "test_frames_features, test_frames_labels = process_images_from_directory(directory)\n",
        "\n",
        "test_frames_labels = to_categorical(test_frames_labels, num_classes=2)"
      ],
      "metadata": {
        "id": "RkPimjUo6GW9"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_frames_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnJI1FEI91gH",
        "outputId": "13e71262-df82-49d4-fd59-412390441ad4"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(57, 64, 64, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_frames_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UPtdpMf-G3m",
        "outputId": "4212e1f6-8757-4a1a-8a1b-62810b5e2214"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(57, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model on test set"
      ],
      "metadata": {
        "id": "z7UunJdx8FEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_frames_features, test_frames_labels)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlfFZ28Q6qbg",
        "outputId": "6c619730-eb7f-49e4-d3aa-87c816eb0659"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 129ms/step - loss: 0.3683 - accuracy: 0.8947\n",
            "Test Accuracy: 89.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "\n",
        "y_pred_proba = model.predict(test_frames_features)\n",
        "predictions_for_frames = []\n",
        "# Output the prediction vector for each test image\n",
        "for idx, prediction_vector in enumerate(y_pred_proba):\n",
        "    print(f\"Prediction vector for test image {idx+1}: {prediction_vector}\")\n",
        "    predictions_for_frames.append((idx, prediction_vector))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivqBLZjo8w3S",
        "outputId": "a3983def-0a24-4911-ac26-7b3323e21309"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 5ms/step\n",
            "Prediction vector for test image 1: [0.6881892  0.31181082]\n",
            "Prediction vector for test image 2: [0.4422481 0.5577519]\n",
            "Prediction vector for test image 3: [0.5819326 0.4180674]\n",
            "Prediction vector for test image 4: [0.95320356 0.04679642]\n",
            "Prediction vector for test image 5: [0.86064345 0.1393566 ]\n",
            "Prediction vector for test image 6: [0.56917477 0.4308252 ]\n",
            "Prediction vector for test image 7: [0.9508233  0.04917675]\n",
            "Prediction vector for test image 8: [0.8650313  0.13496868]\n",
            "Prediction vector for test image 9: [0.6687024  0.33129755]\n",
            "Prediction vector for test image 10: [0.98190844 0.01809159]\n",
            "Prediction vector for test image 11: [0.59112114 0.4088789 ]\n",
            "Prediction vector for test image 12: [0.853582   0.14641802]\n",
            "Prediction vector for test image 13: [0.66795874 0.33204123]\n",
            "Prediction vector for test image 14: [0.5820522  0.41794783]\n",
            "Prediction vector for test image 15: [0.97661954 0.02338043]\n",
            "Prediction vector for test image 16: [0.38508773 0.6149123 ]\n",
            "Prediction vector for test image 17: [0.33789003 0.66211003]\n",
            "Prediction vector for test image 18: [0.7019911  0.29800892]\n",
            "Prediction vector for test image 19: [0.5722001  0.42779985]\n",
            "Prediction vector for test image 20: [0.5832751  0.41672492]\n",
            "Prediction vector for test image 21: [0.818656 0.181344]\n",
            "Prediction vector for test image 22: [0.69685787 0.30314216]\n",
            "Prediction vector for test image 23: [0.9380302  0.06196983]\n",
            "Prediction vector for test image 24: [0.5913216 0.4086784]\n",
            "Prediction vector for test image 25: [0.42219645 0.57780355]\n",
            "Prediction vector for test image 26: [0.61407846 0.3859215 ]\n",
            "Prediction vector for test image 27: [0.9532715  0.04672851]\n",
            "Prediction vector for test image 28: [0.98759776 0.01240222]\n",
            "Prediction vector for test image 29: [0.9579118  0.04208816]\n",
            "Prediction vector for test image 30: [0.5920279  0.40797207]\n",
            "Prediction vector for test image 31: [0.9404057  0.05959429]\n",
            "Prediction vector for test image 32: [0.81637126 0.18362875]\n",
            "Prediction vector for test image 33: [0.60515153 0.39484847]\n",
            "Prediction vector for test image 34: [0.9212672  0.07873278]\n",
            "Prediction vector for test image 35: [0.983493   0.01650703]\n",
            "Prediction vector for test image 36: [0.5985164 0.4014836]\n",
            "Prediction vector for test image 37: [0.895959   0.10404103]\n",
            "Prediction vector for test image 38: [0.65336084 0.3466392 ]\n",
            "Prediction vector for test image 39: [0.60454863 0.3954514 ]\n",
            "Prediction vector for test image 40: [0.59723604 0.4027639 ]\n",
            "Prediction vector for test image 41: [0.97324014 0.0267599 ]\n",
            "Prediction vector for test image 42: [0.727188 0.272812]\n",
            "Prediction vector for test image 43: [0.4717094  0.52829057]\n",
            "Prediction vector for test image 44: [0.94923586 0.05076416]\n",
            "Prediction vector for test image 45: [0.7285032 0.2714968]\n",
            "Prediction vector for test image 46: [0.6256948  0.37430513]\n",
            "Prediction vector for test image 47: [0.59272695 0.40727305]\n",
            "Prediction vector for test image 48: [0.9804497  0.01955032]\n",
            "Prediction vector for test image 49: [0.9846006  0.01539944]\n",
            "Prediction vector for test image 50: [0.32936767 0.6706323 ]\n",
            "Prediction vector for test image 51: [0.5710799  0.42892006]\n",
            "Prediction vector for test image 52: [0.712938   0.28706205]\n",
            "Prediction vector for test image 53: [0.74538016 0.25461987]\n",
            "Prediction vector for test image 54: [0.6377219 0.3622781]\n",
            "Prediction vector for test image 55: [0.6279088  0.37209114]\n",
            "Prediction vector for test image 56: [0.5629397  0.43706027]\n",
            "Prediction vector for test image 57: [0.67565256 0.32434738]\n"
          ]
        }
      ]
    }
  ]
}